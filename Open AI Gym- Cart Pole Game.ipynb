{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Some Common Terms\n",
    "\n",
    "* Agent\n",
    "* Environment\n",
    "* Actions, Rewards, Observations\n",
    "\n",
    "# 1. Interacting with the Gym API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each environment comes with certain import methods/attributes\n",
    "\n",
    "* action_space\n",
    "* observation_space\n",
    "* reset() : returns init state and also resets the environment\n",
    "* step()\n",
    "* render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RenderGame(e):\n",
    "    istate = env.reset() # Initial State\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation,reward,done,info = env.step(action)\n",
    "        #print(observation,reward,info)\n",
    "        if done:\n",
    "            print(\"Game Episode : {}/{} \\nHigh Score : {}\".format(e,20,t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 1/20 \n",
      "High Score : 9\n"
     ]
    }
   ],
   "source": [
    "RenderGame(1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Playing Games with a Random Strategy\n",
    "* Game Episode\n",
    "* Step() Function in More Detail\n",
    "* Game Over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 1/20 \n",
      "High Score : 34\n",
      "Game Episode : 2/20 \n",
      "High Score : 14\n",
      "Game Episode : 3/20 \n",
      "High Score : 13\n",
      "Game Episode : 4/20 \n",
      "High Score : 16\n",
      "Game Episode : 5/20 \n",
      "High Score : 33\n",
      "Game Episode : 6/20 \n",
      "High Score : 33\n",
      "Game Episode : 7/20 \n",
      "High Score : 19\n",
      "Game Episode : 8/20 \n",
      "High Score : 19\n",
      "Game Episode : 9/20 \n",
      "High Score : 20\n",
      "Game Episode : 10/20 \n",
      "High Score : 22\n",
      "Game Episode : 11/20 \n",
      "High Score : 8\n",
      "Game Episode : 12/20 \n",
      "High Score : 10\n",
      "Game Episode : 13/20 \n",
      "High Score : 31\n",
      "Game Episode : 14/20 \n",
      "High Score : 21\n",
      "Game Episode : 15/20 \n",
      "High Score : 13\n",
      "Game Episode : 16/20 \n",
      "High Score : 13\n",
      "Game Episode : 17/20 \n",
      "High Score : 15\n",
      "Game Episode : 18/20 \n",
      "High Score : 31\n",
      "Game Episode : 19/20 \n",
      "High Score : 10\n",
      "Game Episode : 20/20 \n",
      "High Score : 17\n",
      "Game Over!!\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    RenderGame(e+1)\n",
    "env.close()\n",
    "print(\"Game Over!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Q-Learning\n",
    "### Designing an AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.gamma = 0.95\n",
    "        # Exploration vs Explotation TradeOff\n",
    "        # Exploration : Good in the beginning --> helps you to try various random things\n",
    "        # Explotation : Sample Good Experience from past -->good in the end\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = self.create_model()\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim = 4,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(2,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def act(self,state):\n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "        \n",
    "    def train(self,batch_size=32):\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for experience in minibatch:\n",
    "            state,action,reward,next_state,done = experience\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "                \n",
    "            y = self.model.predict(state)\n",
    "            y[0][action] = target\n",
    "            \n",
    "            self.model.fit(x=state,y=y,epochs = 1,verbose=0)\n",
    "            \n",
    "        if self.epsilon>self.epsilon_min:\n",
    "            self.epsilon*=self.epsilon_decay\n",
    "    \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training the DQN Agent (Deep Q-Learner)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "output_dir = \"cartpole_model/\"\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size,action_size=action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 1/1000 \n",
      "High Score : 20 \n",
      "Exploration Rate : 1.0\n",
      "Game Episode : 2/1000 \n",
      "High Score : 29 \n",
      "Exploration Rate : 0.91\n",
      "Game Episode : 3/1000 \n",
      "High Score : 17 \n",
      "Exploration Rate : 0.84\n",
      "Game Episode : 4/1000 \n",
      "High Score : 18 \n",
      "Exploration Rate : 0.77\n",
      "Game Episode : 5/1000 \n",
      "High Score : 12 \n",
      "Exploration Rate : 0.72\n",
      "Game Episode : 6/1000 \n",
      "High Score : 25 \n",
      "Exploration Rate : 0.64\n",
      "Game Episode : 7/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.61\n",
      "Game Episode : 8/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.58\n",
      "Game Episode : 9/1000 \n",
      "High Score : 14 \n",
      "Exploration Rate : 0.54\n",
      "Game Episode : 10/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.52\n",
      "Game Episode : 11/1000 \n",
      "High Score : 14 \n",
      "Exploration Rate : 0.48\n",
      "Game Episode : 12/1000 \n",
      "High Score : 21 \n",
      "Exploration Rate : 0.43\n",
      "Game Episode : 13/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.42\n",
      "Game Episode : 14/1000 \n",
      "High Score : 12 \n",
      "Exploration Rate : 0.39\n",
      "Game Episode : 15/1000 \n",
      "High Score : 13 \n",
      "Exploration Rate : 0.37\n",
      "Game Episode : 16/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.35\n",
      "Game Episode : 17/1000 \n",
      "High Score : 15 \n",
      "Exploration Rate : 0.33\n",
      "Game Episode : 18/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.31\n",
      "Game Episode : 19/1000 \n",
      "High Score : 11 \n",
      "Exploration Rate : 0.3\n",
      "Game Episode : 20/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.28\n",
      "Game Episode : 21/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.27\n",
      "Game Episode : 22/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.26\n",
      "Game Episode : 23/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.25\n",
      "Game Episode : 24/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.23\n",
      "Game Episode : 25/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.22\n",
      "Game Episode : 26/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.21\n",
      "Game Episode : 27/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.2\n",
      "Game Episode : 28/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.19\n",
      "Game Episode : 29/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.19\n",
      "Game Episode : 30/1000 \n",
      "High Score : 12 \n",
      "Exploration Rate : 0.17\n",
      "Game Episode : 31/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.17\n",
      "Game Episode : 32/1000 \n",
      "High Score : 11 \n",
      "Exploration Rate : 0.16\n",
      "Game Episode : 33/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.15\n",
      "Game Episode : 34/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.14\n",
      "Game Episode : 35/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.14\n",
      "Game Episode : 36/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.13\n",
      "Game Episode : 37/1000 \n",
      "High Score : 11 \n",
      "Exploration Rate : 0.12\n",
      "Game Episode : 38/1000 \n",
      "High Score : 11 \n",
      "Exploration Rate : 0.12\n",
      "Game Episode : 39/1000 \n",
      "High Score : 17 \n",
      "Exploration Rate : 0.11\n",
      "Game Episode : 40/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.1\n",
      "Game Episode : 41/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.1\n",
      "Game Episode : 42/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.095\n",
      "Game Episode : 43/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.091\n",
      "Game Episode : 44/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.087\n",
      "Game Episode : 45/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.083\n",
      "Game Episode : 46/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.08\n",
      "Game Episode : 47/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.077\n",
      "Game Episode : 48/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.074\n",
      "Game Episode : 49/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.071\n",
      "Game Episode : 50/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.067\n",
      "Game Episode : 51/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.065\n",
      "Game Episode : 52/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.062\n",
      "Game Episode : 53/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.059\n",
      "Game Episode : 54/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.057\n",
      "Game Episode : 55/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.054\n",
      "Game Episode : 56/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.052\n",
      "Game Episode : 57/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.05\n",
      "Game Episode : 58/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.048\n",
      "Game Episode : 59/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.046\n",
      "Game Episode : 60/1000 \n",
      "High Score : 12 \n",
      "Exploration Rate : 0.043\n",
      "Game Episode : 61/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.041\n",
      "Game Episode : 62/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.04\n",
      "Game Episode : 63/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.038\n",
      "Game Episode : 64/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.037\n",
      "Game Episode : 65/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.035\n",
      "Game Episode : 66/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.034\n",
      "Game Episode : 67/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.032\n",
      "Game Episode : 68/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.031\n",
      "Game Episode : 69/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.03\n",
      "Game Episode : 70/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.029\n",
      "Game Episode : 71/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.027\n",
      "Game Episode : 72/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.026\n",
      "Game Episode : 73/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.025\n",
      "Game Episode : 74/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.024\n",
      "Game Episode : 75/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.023\n",
      "Game Episode : 76/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.022\n",
      "Game Episode : 77/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.021\n",
      "Game Episode : 78/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.02\n",
      "Game Episode : 79/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.019\n",
      "Game Episode : 80/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.019\n",
      "Game Episode : 81/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.018\n",
      "Game Episode : 82/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.017\n",
      "Game Episode : 83/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.016\n",
      "Game Episode : 84/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.016\n",
      "Game Episode : 85/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.015\n",
      "Game Episode : 86/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.014\n",
      "Game Episode : 87/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.014\n",
      "Game Episode : 88/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.013\n",
      "Game Episode : 89/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.013\n",
      "Game Episode : 90/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.012\n",
      "Game Episode : 91/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.012\n",
      "Game Episode : 92/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.011\n",
      "Game Episode : 93/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.011\n",
      "Game Episode : 94/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 95/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 96/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 97/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 98/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 99/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 100/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 101/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 102/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 103/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 104/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 105/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 106/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 107/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 108/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 109/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 110/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 111/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 112/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 113/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 114/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 115/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 116/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 117/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 118/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 119/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 120/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 121/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 122/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 123/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 124/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 125/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 126/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 127/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 128/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 129/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 130/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 131/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 132/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 133/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 134/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 135/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 136/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 137/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 138/1000 \n",
      "High Score : 10 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 139/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 140/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 141/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 142/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 143/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 144/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 145/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 146/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 147/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 148/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 149/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 150/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 151/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 152/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 153/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 154/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 155/1000 \n",
      "High Score : 7 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 156/1000 \n",
      "High Score : 9 \n",
      "Exploration Rate : 0.01\n",
      "Game Episode : 157/1000 \n",
      "High Score : 8 \n",
      "Exploration Rate : 0.01\n"
     ]
    }
   ],
   "source": [
    "for e in range(1,n_episodes+1):\n",
    "    istate = env.reset()\n",
    "    istate = np.reshape(istate,[1,state_size])\n",
    "    \n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        action = agent.act(istate)\n",
    "        next_state,reward,done,info = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(istate,action,reward,next_state,done)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode : {}/{} \\nHigh Score : {} \\nExploration Rate : {:.2}\".format(e,n_episodes,t,agent.epsilon))\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory)>batch_size:\n",
    "            agent.train(batch_size=batch_size)\n",
    "        \n",
    "        if e%50==0:\n",
    "            agent.save(output_dir+\"weights_{:04d}\".format(e)+\".hdf5\")\n",
    "            \n",
    "print(\"Deep Q-Learning Model Trained\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
