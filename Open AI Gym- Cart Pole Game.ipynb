{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Some Common Terms\n",
    "\n",
    "* Agent\n",
    "* Environment\n",
    "* Actions, Rewards, Observations\n",
    "\n",
    "# 1. Interacting with the Gym API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each environment comes with certain import methods/attributes\n",
    "\n",
    "* action_space\n",
    "* observation_space\n",
    "* reset() : returns init state and also resets the environment\n",
    "* step()\n",
    "* render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RenderGame(e):\n",
    "    istate = env.reset() # Initial State\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation,reward,done,info = env.step(action)\n",
    "        #print(observation,reward,info)\n",
    "        if done:\n",
    "            print(\"Game Episode : {}/{} \\nHigh Score : {}\".format(e,20,t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 1/20 \n",
      "High Score : 32\n"
     ]
    }
   ],
   "source": [
    "RenderGame(1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Playing Games with a Random Strategy\n",
    "* Game Episode\n",
    "* Step() Function in More Detail\n",
    "* Game Over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 1/20 \n",
      "High Score : 12\n",
      "Game Episode : 2/20 \n",
      "High Score : 14\n",
      "Game Episode : 3/20 \n",
      "High Score : 32\n",
      "Game Episode : 4/20 \n",
      "High Score : 24\n",
      "Game Episode : 5/20 \n",
      "High Score : 14\n",
      "Game Episode : 7/20 \n",
      "High Score : 22\n",
      "Game Episode : 8/20 \n",
      "High Score : 30\n",
      "Game Episode : 9/20 \n",
      "High Score : 8\n",
      "Game Episode : 10/20 \n",
      "High Score : 17\n",
      "Game Episode : 11/20 \n",
      "High Score : 20\n",
      "Game Episode : 12/20 \n",
      "High Score : 9\n",
      "Game Episode : 13/20 \n",
      "High Score : 10\n",
      "Game Episode : 14/20 \n",
      "High Score : 17\n",
      "Game Episode : 15/20 \n",
      "High Score : 13\n",
      "Game Episode : 16/20 \n",
      "High Score : 26\n",
      "Game Episode : 17/20 \n",
      "High Score : 12\n",
      "Game Episode : 18/20 \n",
      "High Score : 16\n",
      "Game Episode : 19/20 \n",
      "High Score : 17\n",
      "Game Episode : 20/20 \n",
      "High Score : 31\n",
      "Game Over!!\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    RenderGame(e+1)\n",
    "env.close()\n",
    "print(\"Game Over!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Q-Learning\n",
    "### Designing an AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.gamma = 0.95\n",
    "        # Exploration vs Explotation TradeOff\n",
    "        # Exploration : Good in the beginning --> helps you to try various random things\n",
    "        # Explotation : Sample Good Experience from past -->good in the end\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = self.create_model()\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim = self.state_size,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def act(self,state):\n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "        \n",
    "    def train(self,batch_size=32):\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for experience in minibatch:\n",
    "            state,action,reward,next_state,done = experience\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "                \n",
    "            y = self.model.predict(state)\n",
    "            y[0][action] = target\n",
    "            \n",
    "            self.model.fit(x=state,y=y,epochs = 1,verbose=0)\n",
    "            \n",
    "        if self.epsilon>self.epsilon_min:\n",
    "            self.epsilon*=self.epsilon_decay\n",
    "    \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training the DQN Agent (Deep Q-Learner)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "output_dir = \"cartpole_model/\"\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size,action_size=action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 1/1000 High Score : 10 Exploration Rate : 1.0\n",
      "Game Episode : 2/1000 High Score : 27 Exploration Rate : 1.0\n",
      "Game Episode : 3/1000 High Score : 25 Exploration Rate : 0.99\n",
      "Game Episode : 4/1000 High Score : 40 Exploration Rate : 0.99\n",
      "Game Episode : 5/1000 High Score : 23 Exploration Rate : 0.99\n",
      "Game Episode : 6/1000 High Score : 15 Exploration Rate : 0.98\n",
      "Game Episode : 7/1000 High Score : 29 Exploration Rate : 0.98\n",
      "Game Episode : 8/1000 High Score : 10 Exploration Rate : 0.97\n",
      "Game Episode : 9/1000 High Score : 16 Exploration Rate : 0.97\n",
      "Game Episode : 10/1000 High Score : 12 Exploration Rate : 0.96\n",
      "Game Episode : 11/1000 High Score : 15 Exploration Rate : 0.96\n",
      "Game Episode : 12/1000 High Score : 40 Exploration Rate : 0.95\n",
      "Game Episode : 13/1000 High Score : 11 Exploration Rate : 0.95\n",
      "Game Episode : 14/1000 High Score : 16 Exploration Rate : 0.94\n",
      "Game Episode : 15/1000 High Score : 13 Exploration Rate : 0.94\n",
      "Game Episode : 16/1000 High Score : 23 Exploration Rate : 0.93\n",
      "Game Episode : 17/1000 High Score : 53 Exploration Rate : 0.93\n",
      "Game Episode : 18/1000 High Score : 16 Exploration Rate : 0.92\n",
      "Game Episode : 19/1000 High Score : 31 Exploration Rate : 0.92\n",
      "Game Episode : 20/1000 High Score : 19 Exploration Rate : 0.91\n",
      "Game Episode : 21/1000 High Score : 37 Exploration Rate : 0.91\n",
      "Game Episode : 22/1000 High Score : 20 Exploration Rate : 0.9\n",
      "Game Episode : 23/1000 High Score : 13 Exploration Rate : 0.9\n",
      "Game Episode : 24/1000 High Score : 13 Exploration Rate : 0.9\n",
      "Game Episode : 25/1000 High Score : 22 Exploration Rate : 0.89\n",
      "Game Episode : 26/1000 High Score : 30 Exploration Rate : 0.89\n",
      "Game Episode : 27/1000 High Score : 20 Exploration Rate : 0.88\n",
      "Game Episode : 28/1000 High Score : 60 Exploration Rate : 0.88\n",
      "Game Episode : 29/1000 High Score : 28 Exploration Rate : 0.87\n",
      "Game Episode : 30/1000 High Score : 16 Exploration Rate : 0.87\n",
      "Game Episode : 31/1000 High Score : 18 Exploration Rate : 0.86\n",
      "Game Episode : 32/1000 High Score : 53 Exploration Rate : 0.86\n",
      "Game Episode : 33/1000 High Score : 20 Exploration Rate : 0.86\n",
      "Game Episode : 34/1000 High Score : 17 Exploration Rate : 0.85\n",
      "Game Episode : 35/1000 High Score : 27 Exploration Rate : 0.85\n",
      "Game Episode : 36/1000 High Score : 13 Exploration Rate : 0.84\n",
      "Game Episode : 37/1000 High Score : 10 Exploration Rate : 0.84\n",
      "Game Episode : 38/1000 High Score : 14 Exploration Rate : 0.83\n",
      "Game Episode : 39/1000 High Score : 11 Exploration Rate : 0.83\n",
      "Game Episode : 40/1000 High Score : 28 Exploration Rate : 0.83\n",
      "Game Episode : 41/1000 High Score : 30 Exploration Rate : 0.82\n",
      "Game Episode : 42/1000 High Score : 17 Exploration Rate : 0.82\n",
      "Game Episode : 43/1000 High Score : 14 Exploration Rate : 0.81\n",
      "Game Episode : 44/1000 High Score : 16 Exploration Rate : 0.81\n",
      "Game Episode : 45/1000 High Score : 16 Exploration Rate : 0.81\n",
      "Game Episode : 46/1000 High Score : 25 Exploration Rate : 0.8\n",
      "Game Episode : 47/1000 High Score : 12 Exploration Rate : 0.8\n",
      "Game Episode : 48/1000 High Score : 38 Exploration Rate : 0.79\n",
      "Game Episode : 49/1000 High Score : 15 Exploration Rate : 0.79\n",
      "Game Episode : 50/1000 High Score : 12 Exploration Rate : 0.79\n",
      "Game Episode : 51/1000 High Score : 29 Exploration Rate : 0.78\n",
      "Game Episode : 52/1000 High Score : 16 Exploration Rate : 0.78\n",
      "Game Episode : 53/1000 High Score : 15 Exploration Rate : 0.77\n",
      "Game Episode : 54/1000 High Score : 16 Exploration Rate : 0.77\n",
      "Game Episode : 55/1000 High Score : 14 Exploration Rate : 0.77\n",
      "Game Episode : 56/1000 High Score : 20 Exploration Rate : 0.76\n",
      "Game Episode : 57/1000 High Score : 12 Exploration Rate : 0.76\n",
      "Game Episode : 58/1000 High Score : 12 Exploration Rate : 0.76\n",
      "Game Episode : 59/1000 High Score : 34 Exploration Rate : 0.75\n",
      "Game Episode : 60/1000 High Score : 12 Exploration Rate : 0.75\n",
      "Game Episode : 61/1000 High Score : 11 Exploration Rate : 0.74\n",
      "Game Episode : 62/1000 High Score : 12 Exploration Rate : 0.74\n",
      "Game Episode : 63/1000 High Score : 31 Exploration Rate : 0.74\n",
      "Game Episode : 64/1000 High Score : 19 Exploration Rate : 0.73\n",
      "Game Episode : 65/1000 High Score : 16 Exploration Rate : 0.73\n",
      "Game Episode : 66/1000 High Score : 17 Exploration Rate : 0.73\n",
      "Game Episode : 67/1000 High Score : 26 Exploration Rate : 0.72\n",
      "Game Episode : 68/1000 High Score : 29 Exploration Rate : 0.72\n",
      "Game Episode : 69/1000 High Score : 14 Exploration Rate : 0.71\n",
      "Game Episode : 70/1000 High Score : 26 Exploration Rate : 0.71\n",
      "Game Episode : 71/1000 High Score : 16 Exploration Rate : 0.71\n",
      "Game Episode : 72/1000 High Score : 13 Exploration Rate : 0.7\n",
      "Game Episode : 73/1000 High Score : 38 Exploration Rate : 0.7\n",
      "Game Episode : 74/1000 High Score : 21 Exploration Rate : 0.7\n",
      "Game Episode : 75/1000 High Score : 11 Exploration Rate : 0.69\n",
      "Game Episode : 76/1000 High Score : 11 Exploration Rate : 0.69\n",
      "Game Episode : 77/1000 High Score : 31 Exploration Rate : 0.69\n",
      "Game Episode : 78/1000 High Score : 16 Exploration Rate : 0.68\n",
      "Game Episode : 79/1000 High Score : 11 Exploration Rate : 0.68\n",
      "Game Episode : 80/1000 High Score : 13 Exploration Rate : 0.68\n",
      "Game Episode : 81/1000 High Score : 15 Exploration Rate : 0.67\n",
      "Game Episode : 82/1000 High Score : 12 Exploration Rate : 0.67\n",
      "Game Episode : 83/1000 High Score : 17 Exploration Rate : 0.67\n",
      "Game Episode : 84/1000 High Score : 20 Exploration Rate : 0.66\n",
      "Game Episode : 85/1000 High Score : 11 Exploration Rate : 0.66\n",
      "Game Episode : 86/1000 High Score : 20 Exploration Rate : 0.66\n",
      "Game Episode : 87/1000 High Score : 14 Exploration Rate : 0.65\n",
      "Game Episode : 88/1000 High Score : 12 Exploration Rate : 0.65\n",
      "Game Episode : 89/1000 High Score : 18 Exploration Rate : 0.65\n",
      "Game Episode : 90/1000 High Score : 12 Exploration Rate : 0.64\n",
      "Game Episode : 91/1000 High Score : 10 Exploration Rate : 0.64\n",
      "Game Episode : 92/1000 High Score : 15 Exploration Rate : 0.64\n",
      "Game Episode : 93/1000 High Score : 15 Exploration Rate : 0.63\n",
      "Game Episode : 94/1000 High Score : 22 Exploration Rate : 0.63\n",
      "Game Episode : 95/1000 High Score : 25 Exploration Rate : 0.63\n",
      "Game Episode : 96/1000 High Score : 11 Exploration Rate : 0.62\n",
      "Game Episode : 97/1000 High Score : 32 Exploration Rate : 0.62\n",
      "Game Episode : 98/1000 High Score : 18 Exploration Rate : 0.62\n",
      "Game Episode : 99/1000 High Score : 42 Exploration Rate : 0.61\n",
      "Game Episode : 100/1000 High Score : 13 Exploration Rate : 0.61\n",
      "Game Episode : 101/1000 High Score : 26 Exploration Rate : 0.61\n",
      "Game Episode : 102/1000 High Score : 15 Exploration Rate : 0.61\n",
      "Game Episode : 103/1000 High Score : 24 Exploration Rate : 0.6\n",
      "Game Episode : 104/1000 High Score : 10 Exploration Rate : 0.6\n",
      "Game Episode : 105/1000 High Score : 19 Exploration Rate : 0.6\n",
      "Game Episode : 106/1000 High Score : 16 Exploration Rate : 0.59\n",
      "Game Episode : 107/1000 High Score : 34 Exploration Rate : 0.59\n",
      "Game Episode : 108/1000 High Score : 23 Exploration Rate : 0.59\n",
      "Game Episode : 109/1000 High Score : 110 Exploration Rate : 0.58\n",
      "Game Episode : 110/1000 High Score : 70 Exploration Rate : 0.58\n",
      "Game Episode : 111/1000 High Score : 19 Exploration Rate : 0.58\n",
      "Game Episode : 112/1000 High Score : 16 Exploration Rate : 0.58\n",
      "Game Episode : 113/1000 High Score : 23 Exploration Rate : 0.57\n",
      "Game Episode : 114/1000 High Score : 24 Exploration Rate : 0.57\n",
      "Game Episode : 115/1000 High Score : 15 Exploration Rate : 0.57\n",
      "Game Episode : 116/1000 High Score : 25 Exploration Rate : 0.56\n",
      "Game Episode : 117/1000 High Score : 40 Exploration Rate : 0.56\n",
      "Game Episode : 118/1000 High Score : 61 Exploration Rate : 0.56\n",
      "Game Episode : 119/1000 High Score : 50 Exploration Rate : 0.56\n",
      "Game Episode : 120/1000 High Score : 78 Exploration Rate : 0.55\n",
      "Game Episode : 121/1000 High Score : 25 Exploration Rate : 0.55\n",
      "Game Episode : 122/1000 High Score : 38 Exploration Rate : 0.55\n",
      "Game Episode : 123/1000 High Score : 91 Exploration Rate : 0.55\n",
      "Game Episode : 124/1000 High Score : 56 Exploration Rate : 0.54\n",
      "Game Episode : 125/1000 High Score : 21 Exploration Rate : 0.54\n",
      "Game Episode : 126/1000 High Score : 38 Exploration Rate : 0.54\n",
      "Game Episode : 127/1000 High Score : 11 Exploration Rate : 0.53\n",
      "Game Episode : 128/1000 High Score : 22 Exploration Rate : 0.53\n",
      "Game Episode : 129/1000 High Score : 44 Exploration Rate : 0.53\n",
      "Game Episode : 130/1000 High Score : 82 Exploration Rate : 0.53\n",
      "Game Episode : 131/1000 High Score : 45 Exploration Rate : 0.52\n",
      "Game Episode : 132/1000 High Score : 28 Exploration Rate : 0.52\n",
      "Game Episode : 133/1000 High Score : 95 Exploration Rate : 0.52\n",
      "Game Episode : 134/1000 High Score : 99 Exploration Rate : 0.52\n",
      "Game Episode : 135/1000 High Score : 35 Exploration Rate : 0.51\n",
      "Game Episode : 136/1000 High Score : 14 Exploration Rate : 0.51\n",
      "Game Episode : 137/1000 High Score : 38 Exploration Rate : 0.51\n",
      "Game Episode : 138/1000 High Score : 25 Exploration Rate : 0.51\n",
      "Game Episode : 139/1000 High Score : 53 Exploration Rate : 0.5\n",
      "Game Episode : 140/1000 High Score : 72 Exploration Rate : 0.5\n",
      "Game Episode : 141/1000 High Score : 23 Exploration Rate : 0.5\n",
      "Game Episode : 142/1000 High Score : 107 Exploration Rate : 0.5\n",
      "Game Episode : 143/1000 High Score : 40 Exploration Rate : 0.49\n",
      "Game Episode : 144/1000 High Score : 19 Exploration Rate : 0.49\n",
      "Game Episode : 145/1000 High Score : 48 Exploration Rate : 0.49\n",
      "Game Episode : 146/1000 High Score : 28 Exploration Rate : 0.49\n",
      "Game Episode : 147/1000 High Score : 98 Exploration Rate : 0.48\n",
      "Game Episode : 148/1000 High Score : 28 Exploration Rate : 0.48\n",
      "Game Episode : 149/1000 High Score : 23 Exploration Rate : 0.48\n",
      "Game Episode : 150/1000 High Score : 42 Exploration Rate : 0.48\n",
      "Game Episode : 151/1000 High Score : 27 Exploration Rate : 0.47\n",
      "Game Episode : 152/1000 High Score : 34 Exploration Rate : 0.47\n",
      "Game Episode : 153/1000 High Score : 38 Exploration Rate : 0.47\n",
      "Game Episode : 154/1000 High Score : 41 Exploration Rate : 0.47\n",
      "Game Episode : 155/1000 High Score : 33 Exploration Rate : 0.46\n",
      "Game Episode : 156/1000 High Score : 23 Exploration Rate : 0.46\n",
      "Game Episode : 157/1000 High Score : 23 Exploration Rate : 0.46\n",
      "Game Episode : 158/1000 High Score : 40 Exploration Rate : 0.46\n",
      "Game Episode : 159/1000 High Score : 34 Exploration Rate : 0.46\n",
      "Game Episode : 160/1000 High Score : 51 Exploration Rate : 0.45\n",
      "Game Episode : 161/1000 High Score : 45 Exploration Rate : 0.45\n",
      "Game Episode : 162/1000 High Score : 44 Exploration Rate : 0.45\n",
      "Game Episode : 163/1000 High Score : 88 Exploration Rate : 0.45\n",
      "Game Episode : 164/1000 High Score : 61 Exploration Rate : 0.44\n",
      "Game Episode : 165/1000 High Score : 50 Exploration Rate : 0.44\n",
      "Game Episode : 166/1000 High Score : 54 Exploration Rate : 0.44\n",
      "Game Episode : 167/1000 High Score : 51 Exploration Rate : 0.44\n",
      "Game Episode : 168/1000 High Score : 52 Exploration Rate : 0.44\n",
      "Game Episode : 169/1000 High Score : 40 Exploration Rate : 0.43\n",
      "Game Episode : 170/1000 High Score : 139 Exploration Rate : 0.43\n",
      "Game Episode : 171/1000 High Score : 85 Exploration Rate : 0.43\n",
      "Game Episode : 172/1000 High Score : 126 Exploration Rate : 0.43\n",
      "Game Episode : 173/1000 High Score : 52 Exploration Rate : 0.42\n",
      "Game Episode : 174/1000 High Score : 66 Exploration Rate : 0.42\n",
      "Game Episode : 175/1000 High Score : 39 Exploration Rate : 0.42\n",
      "Game Episode : 176/1000 High Score : 36 Exploration Rate : 0.42\n",
      "Game Episode : 177/1000 High Score : 87 Exploration Rate : 0.42\n",
      "Game Episode : 178/1000 High Score : 31 Exploration Rate : 0.41\n",
      "Game Episode : 179/1000 High Score : 50 Exploration Rate : 0.41\n",
      "Game Episode : 180/1000 High Score : 200 Exploration Rate : 0.41\n",
      "Deep Q-Learning Model Trained\n"
     ]
    }
   ],
   "source": [
    "for e in range(1,n_episodes+1):\n",
    "    istate = env.reset()\n",
    "    istate = np.reshape(istate,[1,state_size])\n",
    "    \n",
    "    for t in range(1,300):\n",
    "        env.render()\n",
    "        action = agent.act(istate)\n",
    "        next_state,reward,done,info = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(istate,action,reward,next_state,done)\n",
    "        istate = next_state\n",
    "        \n",
    "        if done or t==200:\n",
    "            print(\"Game Episode : {}/{} High Score : {} Exploration Rate : {:.2}\".format(e,n_episodes,t,agent.epsilon))\n",
    "            break\n",
    "        \n",
    "    if len(agent.memory)>batch_size:\n",
    "        agent.train(batch_size=batch_size)\n",
    "        \n",
    "    if e%50==0 or t==200:\n",
    "        agent.save(output_dir+\"weights_{:04d}\".format(e)+\".hdf5\")\n",
    "    \n",
    "    if t==200:\n",
    "        break\n",
    "            \n",
    "print(\"Deep Q-Learning Model Trained\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
